{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d3b7f9",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis â€” Week 4 Task 4\n",
    "\n",
    "**Deliverable:** Notebook showcasing data preprocessing, model implementation, and insights.\n",
    "\n",
    "\n",
    "This notebook contains:\n",
    "\n",
    "\n",
    "1. Data ingestion (uses a small synthetic dataset if a CSV is not provided).\n",
    "2. Text preprocessing and exploratory checks.\n",
    "3. Model pipeline using TF-IDF + Logistic Regression.\n",
    "4. Evaluation (accuracy, precision/recall/F1, confusion matrix).\n",
    "5. Notes & next steps for improvement.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d12d1c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data\n",
    "\n",
    "- The notebook looks for a `data.csv` in the working directory with columns `text` and `label`.\n",
    "- If no file is found, a synthetic demo dataset (tweets/reviews) is created automatically.\n",
    "\n",
    "**Tip:** Replace `data.csv` with your real dataset for the internship task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b42e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo dataset preview\n",
    "import pandas as pd\n",
    "df = pd.read_csv('demo_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23126ba8",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Preprocessing\n",
    "\n",
    "Steps performed:\n",
    "- Lowercasing\n",
    "- Remove URLs and mentions\n",
    "- Remove `#` symbol\n",
    "- Keep alphanumeric characters and basic punctuation (! and ?)\n",
    "- Simple whitespace normalization\n",
    "\n",
    "You can extend this with tokenization, stopword removal, lemmatization, spelling correction, or language detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(text):\n",
    "    import re\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\\\S+', '', text)\n",
    "    text = re.sub(r'@\\\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\\\s\\\\!\\\\?]', '', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply and preview\n",
    "import pandas as pd\n",
    "df = pd.read_csv('demo_data.csv')\n",
    "df['clean_text'] = df['text'].apply(simple_preprocess)\n",
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e8122",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Modeling\n",
    "\n",
    "Pipeline used:\n",
    "- `TfidfVectorizer(ngram_range=(1,2), min_df=2)`\n",
    "- `LogisticRegression(max_iter=1000)`\n",
    "\n",
    "Cross-validated weighted F1 on training set and final evaluation on test set are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebc1018",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, accuracy_score\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=2)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "print('CV weighted F1 (5-fold):', cv_scores.mean())\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print('Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595739ca",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Results & Insights\n",
    "\n",
    "- Cross-validated weighted F1 on training set (5-fold): 1.0000\n",
    "- Test accuracy: 1.0000\n",
    "\n",
    "Confusion matrix and classification report follow.\n",
    "\n",
    "**Insights:**\n",
    "- With synthetic/demo data the model achieves reasonable separation between positive and negative labels.\n",
    "- Neutral examples are fewer and often confused with positive/negative; collecting more neutral samples helps.\n",
    "- Using more advanced preprocessing (lemmatization), handling negation, or using transformer-based embeddings will likely improve performance.\n",
    "\n",
    "**Next steps (recommended):**\n",
    "- Use a larger labeled dataset (e.g., Twitter, Amazon reviews, IMDB) with realistic language variation.\n",
    "- Add class-weighting or resampling if class imbalance is present.\n",
    "- Try transformer models (BERT/RoBERTa) for better contextual understanding.\n",
    "- Add explainability (e.g., LIME/SHAP) to surface which words drive predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bf257e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'confusion_matrix.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Show confusion matrix image\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[1;32m----> 3\u001b[0m display(Image(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfusion_matrix.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\YOGESHWARI\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:1053\u001b[0m, in \u001b[0;36mImage.__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28msuper\u001b[39m(Image, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, url\u001b[38;5;241m=\u001b[39murl, filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1054\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\YOGESHWARI\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:371\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[1;32mc:\\Users\\YOGESHWARI\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:1088\u001b[0m, in \u001b[0;36mImage.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Image,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[1;32mc:\\Users\\YOGESHWARI\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:397\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'confusion_matrix.png'"
     ]
    }
   ],
   "source": [
    "# Show confusion matrix image\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='confusion_matrix.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a9fd5",
   "metadata": {},
   "source": [
    "## Save artifacts\n",
    "The trained pipeline is saved as `sentiment_pipeline.joblib` and a demo dataset `demo_data.csv` is included.\n",
    "\n",
    "---\n",
    "\n",
    "**End of notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
